
@article{zuccon_crowdsourcing_2013,
	title = {Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-012-9206-z},
	doi = {10.1007/s10791-012-9206-z},
	shorttitle = {Crowdsourcing interactions},
	abstract = {In the field of information retrieval ({IR}), researchers and practitioners are often faced with a demand for valid approaches to evaluate the performance of retrieval systems. The Cranfield experiment paradigm has been dominant for the in-vitro evaluation of {IR} systems. Alternative to this paradigm, laboratory-based user studies have been widely used to evaluate interactive information retrieval ({IIR}) systems, and at the same time investigate users’ information searching behaviours. Major drawbacks of laboratory-based user studies for evaluating {IIR} systems include the high monetary and temporal costs involved in setting up and running those experiments, the lack of heterogeneity amongst the user population and the limited scale of the experiments, which usually involve a relatively restricted set of users. In this paper, we propose an alternative experimental methodology to laboratory-based user studies. Our novel experimental methodology uses a crowdsourcing platform as a means of engaging study participants. Through crowdsourcing, our experimental methodology can capture user interactions and searching behaviours at a lower cost, with more data, and within a shorter period than traditional laboratory-based user studies, and therefore can be used to assess the performances of {IIR} systems. In this article, we show the characteristic differences of our approach with respect to traditional {IIR} experimental and evaluation procedures. We also perform a use case study comparing crowdsourcing-based evaluation with laboratory-based evaluation of {IIR} systems, which can serve as a tutorial for setting up crowdsourcing-based {IIR} evaluations.},
	pages = {267--305},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Inf Retrieval},
	author = {Zuccon, Guido and Leelanupab, Teerapong and Whiting, Stewart and Yilmaz, Emine and Jose, Joemon M. and Azzopardi, Leif},
	urldate = {2013-07-11},
	date = {2013-04-01},
	keywords = {Crowdsourcing evaluation, Data Mining and Knowledge Discovery, Document Preparation and Text Processing, Information Storage and Retrieval, Interactions, Interactive {IR} evaluation, Pattern Recognition, Cryptology and Information Theory, Data Structures},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/QTJJ2HTT/Zuccon et al. - 2013 - Crowdsourcing interactions using crowdsourcing fo.pdf:application/pdf;Snapshot:/Users/soboroff/Zotero/storage/SPL362B8/s10791-012-9206-z.html:text/html}
}

@inproceedings{soboroff_comparison_2007,
	title = {A comparison of pooled and sampled relevance judgments},
	isbn = {978-1-59593-597-7},
	url = {http://doi.acm.org/10.1145/1277741.1277908},
	doi = {10.1145/1277741.1277908},
	series = {{SIGIR} '07},
	abstract = {Test collections are most useful when they are reusable, that is, when they can be reliably used to rank systems that did not contribute to the pools. Pooled relevance judgments for very large collections may not be reusable for two easons: they will be very sparse and not sufficiently complete, and they may be biased in the sense that theywill unfairly rank some class of systems. The {TREC} 2006 terabyte track judged both a pool and a deep random sample in order to measure the effects of sparseness and bias.},
	pages = {785--786},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	urldate = {2013-07-11},
	date = {2007},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {pooling, sampling, test collections},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/P54BSXNM/Soboroff - 2007 - A comparison of pooled and sampled relevance judgm.pdf:application/pdf;ACM Full Text PDF:/Users/soboroff/Zotero/storage/DVFP422L/Soboroff - 2007 - A comparison of pooled and sampled relevance judgm.pdf:application/pdf}
}

@inproceedings{soboroff_dynamic_2006,
	title = {Dynamic test collections: measuring search effectiveness on the live web},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148220},
	doi = {10.1145/1148170.1148220},
	series = {{SIGIR} '06},
	shorttitle = {Dynamic test collections},
	pages = {276--283},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	urldate = {2013-07-11},
	date = {2006},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {collections, retrieval, test},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/CBS5E34N/Soboroff - 2006 - Dynamic test collections measuring search effecti.pdf:application/pdf}
}

@article{lease_crowdsourcing_2013,
	title = {Crowdsourcing for information retrieval: introduction to the special issue},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-013-9222-7},
	doi = {10.1007/s10791-013-9222-7},
	shorttitle = {Crowdsourcing for information retrieval},
	abstract = {This introduction to the special issue summarizes and contextualizes six novel research contributions at the intersection of information retrieval ({IR}) and crowdsourcing (also overlapping crowdsourcing’s closely-related sibling, human computation). Several of the papers included in this special issue represent deeper investigations into research topics for which earlier stages of the authors’ research were disseminated at crowdsourcing workshops at {SIGIR} and {WSDM} conferences, as well as at the {NIST} {TREC} conference. Since the first proposed use of crowdsourcing for {IR} in 2008, interest in this area has quickly accelerated and led to three workshops, an ongoing {NIST} {TREC} track, and a great variety of published papers, talks, and tutorials. We briefly summarize the area in order to help situate the contributions appearing in this special issue. We also discuss some broader current trends and issues in crowdsourcing which bear upon its use in {IR} and other fields.},
	pages = {91--100},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Inf Retrieval},
	author = {Lease, Matthew and Yilmaz, Emine},
	urldate = {2013-07-11},
	date = {2013-04-01},
	keywords = {Data Mining and Knowledge Discovery, Document Preparation and Text Processing, Information Storage and Retrieval, Pattern Recognition, crowdsourcing, human computation, Search evaluation, Cryptology and Information Theory, Data Structures},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/NVSAUN3Q/Lease and Yilmaz - 2013 - Crowdsourcing for information retrieval introduct.pdf:application/pdf;Full Text PDF:/Users/soboroff/Zotero/storage/R3PH4L8M/Lease and Yilmaz - 2013 - Crowdsourcing for information retrieval introduct.pdf:application/pdf;Snapshot:/Users/soboroff/Zotero/storage/QIHW9WXN/10.html:text/html;Snapshot:/Users/soboroff/Zotero/storage/5F5XR5IX/10.html:text/html;Snapshot:/Users/soboroff/Zotero/storage/P9N8AZR4/s10791-013-9222-7.html:text/html}
}

@inproceedings{cormack_efficient_1998,
	title = {Efficient construction of large test collections},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291009},
	doi = {10.1145/290941.291009},
	series = {{SIGIR} '98},
	pages = {282--289},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Cormack, Gordon V. and Palmer, Christopher R. and Clarke, Charles L. A.},
	urldate = {2013-07-14},
	date = {1998},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/NKYGPLBR/Cormack et al. - 1998 - Efficient construction of large test collections.pdf:application/pdf}
}

@inproceedings{asadi_pseudo_2011,
	title = {Pseudo test collections for learning web search ranking functions},
	isbn = {978-1-4503-0757-4},
	url = {http://doi.acm.org/10.1145/2009916.2010058},
	doi = {10.1145/2009916.2010058},
	series = {{SIGIR} '11},
	abstract = {Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on {TREC} web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data "for free".},
	pages = {1073--1082},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on Research and development in Information Retrieval},
	publisher = {{ACM}},
	author = {Asadi, Nima and Metzler, Donald and Elsayed, Tamer and Lin, Jimmy},
	urldate = {2013-07-19},
	date = {2011},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {anchor text, evaluation, hyperlinks, web},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/2669R72C/Asadi et al. - 2011 - Pseudo test collections for learning web search ra.pdf:application/pdf}
}

@article{piwowarski_sound_2008,
	title = {Sound and complete relevance assessment for {XML} retrieval},
	volume = {27},
	issn = {1046-8188},
	url = {http://doi.acm.org/10.1145/1416950.1416951},
	doi = {10.1145/1416950.1416951},
	abstract = {In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In {XML} retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results. A major difference between {XML} retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented {XML} retrieval as carried out at {INEX}, the evaluation campaign for {XML} retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents—even though the objective is to collect assessments at element level. The different methods of gathering assessments at {INEX} are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods.},
	pages = {1:1--1:37},
	number = {1},
	journaltitle = {{ACM} Trans. Inf. Syst.},
	author = {Piwowarski, Benjamin and Trotman, Andrew and Lalmas, Mounia},
	urldate = {2013-07-19},
	date = {2008-12},
	keywords = {evaluation, {INEX}, passage retrieval, Relevance assessment, {XML}, {XML} retrieval},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/7IXTWXS4/Piwowarski et al. - 2008 - Sound and complete relevance assessment for XML re.pdf:application/pdf}
}

@report{macdonald_trec_2006,
	title = {The {TREC} Blogs06 Collection : Creating and Analysing a Blog Test Collection},
	url = {http://ir.dcs.gla.ac.uk/terrier/publications/macdonald06creating.pdf},
	number = {{TR}-2006-224},
	institution = {Department of Computing Science, University of Glasgow},
	author = {Macdonald, C. and Ounis, I.},
	date = {2006},
	file = {macdonald06creating.pdf:/Users/soboroff/Zotero/storage/N36KXGYS/macdonald06creating.pdf:application/pdf}
}

@article{kazai_analysis_2013,
	title = {An analysis of human factors and label accuracy in crowdsourcing relevance judgments},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-012-9205-0},
	doi = {10.1007/s10791-012-9205-0},
	abstract = {Crowdsourcing relevance judgments for the evaluation of search engines is used increasingly to overcome the issue of scalability that hinders traditional approaches relying on a fixed group of trusted expert judges. However, the benefits of crowdsourcing come with risks due to the engagement of a self-forming group of individuals—the crowd, motivated by different incentives, who complete the tasks with varying levels of attention and success. This increases the need for a careful design of crowdsourcing tasks that attracts the right crowd for the given task and promotes quality work. In this paper, we describe a series of experiments using Amazon’s Mechanical Turk, conducted to explore the ‘human’ characteristics of the crowds involved in a relevance assessment task. In the experiments, we vary the level of pay offered, the effort required to complete a task and the qualifications required of the workers. We observe the effects of these variables on the quality of the resulting relevance labels, measured based on agreement with a gold set, and correlate them with self-reported measures of various human factors. We elicit information from the workers about their motivations, interest and familiarity with the topic, perceived task difficulty, and satisfaction with the offered pay. We investigate how these factors combine with aspects of the task design and how they affect the accuracy of the resulting relevance labels. Based on the analysis of 960 {HITs} and 2,880 {HIT} assignments resulting in 19,200 relevance labels, we arrive at insights into the complex interaction of the observed factors and provide practical guidelines to crowdsourcing practitioners. In addition, we highlight challenges in the data analysis that stem from the peculiarity of the crowdsourcing environment where the sample of individuals engaged in specific work conditions are inherently influenced by the conditions themselves.},
	pages = {138--178},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Inf Retrieval},
	author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
	urldate = {2013-07-11},
	date = {2013-04-01},
	keywords = {Data Mining and Knowledge Discovery, Document Preparation and Text Processing, Information Storage and Retrieval, Pattern Recognition, crowdsourcing, relevance judgments, Study of human factors, Cryptology and Information Theory, Data Structures},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/5UFSHVIZ/Kazai et al. - 2013 - An analysis of human factors and label accuracy in.pdf:application/pdf;Snapshot:/Users/soboroff/Zotero/storage/FAYJG698/s10791-012-9205-0.html:text/html}
}

@article{alonso_implementing_2013,
	title = {Implementing crowdsourcing-based relevance experimentation: an industrial perspective},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-012-9204-1},
	doi = {10.1007/s10791-012-9204-1},
	shorttitle = {Implementing crowdsourcing-based relevance experimentation},
	abstract = {Crowdsourcing has emerged as a viable platform for conducting different types of relevance evaluation. The main reason behind this trend is that it makes possible to conduct experiments extremely fast, with good results at a low cost. However, like in any experiment, there are several implementation details that would make an experiment work or fail. To gather useful results, clear instructions, user interface guidelines, content quality, inter-rater agreement metrics, work quality, and worker feedback are important characteristics of a successful crowdsourcing experiment. Furthermore, designing and implementing experiments that require thousands or millions of labels is different than conducting small scale research investigations. In this paper we outline a framework for conducting continuous crowdsourcing experiments, emphasizing aspects that should be of importance for all sorts of tasks. We illustrate the value of characteristics that can impact the overall outcome using examples based on {TREC}, {INEX}, and Wikipedia data sets.},
	pages = {101--120},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Inf Retrieval},
	author = {Alonso, Omar},
	urldate = {2013-07-11},
	date = {2013-04-01},
	keywords = {Data Mining and Knowledge Discovery, Document Preparation and Text Processing, Information Storage and Retrieval, Pattern Recognition, crowdsourcing, Experiment design, Methodology, Relevance assessment \& evaluation, Cryptology and Information Theory, Data Structures},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/WCG8RVQ9/Alonso - 2013 - Implementing crowdsourcing-based relevance experim.pdf:application/pdf;Snapshot:/Users/soboroff/Zotero/storage/RX2ZX8FF/s10791-012-9204-1.html:text/html}
}

@article{eickhoff_increasing_2013,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-011-9181-9},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	pages = {121--137},
	number = {2},
	journaltitle = {Information Retrieval},
	shortjournal = {Inf Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P. de},
	urldate = {2013-07-11},
	date = {2013-04-01},
	keywords = {Data Mining and Knowledge Discovery, Document Preparation and Text Processing, Information Storage and Retrieval, Pattern Recognition, crowdsourcing, Human factors, Stability, User experiments, Cryptology and Information Theory, Data Structures},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/GL3TLSGH/Eickhoff and Vries - 2013 - Increasing cheat robustness of crowdsourcing tasks.pdf:application/pdf;Snapshot:/Users/soboroff/Zotero/storage/SUR3NGG6/s10791-011-9181-9.html:text/html}
}

@inproceedings{kazai_analysis_2012,
	title = {An analysis of systematic judging errors in information retrieval},
	isbn = {978-1-4503-1156-4},
	url = {http://doi.acm.org/10.1145/2396761.2396779},
	doi = {10.1145/2396761.2396779},
	series = {{CIKM} '12},
	abstract = {Test collections are powerful mechanisms for the evaluation and optimization of information retrieval systems. However, there is reported evidence that experiment outcomes can be affected by changes to the judging guidelines or changes in the judge population. This paper examines such effects in a web search setting, comparing the judgments of four groups of judges: {NIST} Web Track judges, untrained crowd workers and two groups of trained judges of a commercial search engine. Our goal is to identify systematic judging errors by comparing the labels contributed by the different groups, working under the same or different judging guidelines. In particular, we focus on detecting systematic differences in judging depending on specific characteristics of the queries and {URLs}. For example, we ask whether a given population of judges, working under a given set of judging guidelines, are more likely to consistently overrate Wikipedia pages than another group judging under the same instructions. Our approach is to identify judging errors with respect to a consensus set, a judged gold set and a set of user clicks. We further demonstrate how such biases can affect the training of retrieval systems.},
	pages = {105--114},
	booktitle = {Proceedings of the 21st {ACM} international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Kazai, Gabriella and Craswell, Nick and Yilmaz, Emine and Tahaghoghi, S.M.M},
	urldate = {2013-07-11},
	date = {2012},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {bias, noise, relevence},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/BIM5TB9I/Kazai et al. - 2012 - An analysis of systematic judging errors in inform.pdf:application/pdf}
}

@unpublished{ipeirotis_managing_2011,
	title = {Managing Crowdsourced Human Computation: A Tutorial},
	url = {http://www.slideshare.net/ipeirotis/managing-crowdsourced-human-computation},
	shorttitle = {Managing Crowdsourced Human Computation},
	abstract = {The slides from the tutorial presented during the {WWW}2011 conference in Hyderabad, India (March 29th 2011, by Panos Ipeirotis)},
	author = {Ipeirotis, Panos},
	urldate = {2013-07-11},
	date = {2011-08-02}
}

@inproceedings{sheng_get_2008,
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	series = {{KDD} '08},
	shorttitle = {Get another label?},
	abstract = {This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.},
	pages = {614--622},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	urldate = {2013-07-11},
	date = {2008},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {data preprocessing, data selection},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/GIGVK3LF/Sheng et al. - 2008 - Get another label improving data quality and data.pdf:application/pdf}
}

@inproceedings{ipeirotis_quality_2010,
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	series = {{HCOMP} '10},
	abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings.},
	pages = {64--67},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	urldate = {2013-07-11},
	date = {2010},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/5EZ5BKMF/Ipeirotis et al. - 2010 - Quality management on Amazon Mechanical Turk.pdf:application/pdf}
}

@inproceedings{ipeirotis_managing_2011-1,
	title = {Managing crowdsourced human computation: a tutorial},
	isbn = {978-1-4503-0637-9},
	url = {http://doi.acm.org/10.1145/1963192.1963314},
	doi = {10.1145/1963192.1963314},
	series = {{WWW} '11},
	shorttitle = {Managing crowdsourced human computation},
	abstract = {The tutorial covers an emerging topic of wide interest: Crowdsourcing. Specifically, we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However, most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields, including computer science, statistics, economics, and psychology. Furthermore, the material will include real-life examples and case studies from years of experience in running and managing crowdsourcing applications in business settings.},
	pages = {287--288},
	booktitle = {Proceedings of the 20th international conference companion on World wide web},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Paritosh, Praveen K.},
	urldate = {2013-07-11},
	date = {2011},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {crowdsourcing, human computation, incentives, market design, mechanical turk, quality assurance, reputation, workflow control},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/MSCNBWT5/Ipeirotis and Paritosh - 2011 - Managing crowdsourced human computation a tutoria.pdf:application/pdf}
}

@inproceedings{aslam_inferring_2006,
	title = {Inferring document relevance via average precision},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148275},
	doi = {10.1145/1148170.1148275},
	series = {{SIGIR} '06},
	abstract = {We consider the problem of evaluating retrieval systems using a limited number of relevance judgments. Recent work has demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of documents. In this work, we demonstrate that given values or estimates of average precision, one can accurately infer the relevances of unjudged documents. Combined, we thus show how one can efficiently and accurately infer a large judged pool from a relatively small number of judged documents, thus permitting accurate and efficient retrieval evaluation on a large scale.},
	pages = {601--602},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Yilmaz, Emine},
	urldate = {2013-07-11},
	date = {2006},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {relevance judgments, average precision},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/8BHFD597/Aslam and Yilmaz - 2006 - Inferring document relevance via average precision.pdf:application/pdf}
}

@inproceedings{aslam_statistical_2006,
	title = {A statistical method for system evaluation using incomplete judgments},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148263},
	doi = {10.1145/1148170.1148263},
	series = {{SIGIR} '06},
	abstract = {We consider the problem of large-scale retrieval evaluation, and we propose a statistical method for evaluating retrieval systems using incomplete judgments. Unlike existing techniques that (1) rely on effectively complete, and thus prohibitively expensive, relevance judgment sets, (2) produce biased estimates of standard performance measures, or (3) produce estimates of non-standard measures thought to be correlated with these standard measures, our proposed statistical technique produces unbiased estimates of the standard measures themselves.Our proposed technique is based on random sampling. While our estimates are unbiased by statistical design, their variance is dependent on the sampling distribution employed; as such, we derive a sampling distribution likely to yield low variance estimates. We test our proposed technique using benchmark {TREC} data, demonstrating that a sampling pool derived from a set of runs can be used to efficiently and effectively evaluate those runs. We further show that these sampling pools generalize well to unseen runs. Our experiments indicate that highly accurate estimates of standard performance measures can be obtained using a number of relevance judgments as small as 4\% of the typical {TREC}-style judgment pool.},
	pages = {541--548},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Pavlu, Virgil and Yilmaz, Emine},
	urldate = {2013-07-11},
	date = {2006},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {sampling, evaluation, average precision},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/TL6PRY3Y/Aslam et al. - 2006 - A statistical method for system evaluation using i.pdf:application/pdf}
}

@inproceedings{yilmaz_new_2008,
	title = {A new rank correlation coefficient for information retrieval},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390435},
	doi = {10.1145/1390334.1390435},
	series = {{SIGIR} '08},
	abstract = {In the field of information retrieval, one is often faced with the problem of computing the correlation between two ranked lists. The most commonly used statistic that quantifies this correlation is Kendall's Τ. Often times, in the information retrieval community, discrepancies among those items having high rankings are more important than those among items having low rankings. The Kendall's Τ statistic, however, does not make such distinctions and equally penalizes errors both at high and low rankings. In this paper, we propose a new rank correlation coefficient, {AP} correlation (Τap), that is based on average precision and has a probabilistic interpretation. We show that the proposed statistic gives more weight to the errors at high rankings and has nice mathematical properties which make it easy to interpret. We further validate the applicability of the statistic using experimental data.},
	pages = {587--594},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Aslam, Javed A. and Robertson, Stephen},
	urldate = {2013-07-11},
	date = {2008},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {evaluation, average precision, Kendall's tau, rank correlation},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/G556HQ7J/Yilmaz et al. - 2008 - A new rank correlation coefficient for information.pdf:application/pdf}
}

@inproceedings{aslam_effectiveness_2003,
	title = {On the effectiveness of evaluating retrieval systems in the absence of relevance judgments},
	isbn = {1-58113-646-3},
	url = {http://doi.acm.org/10.1145/860435.860501},
	doi = {10.1145/860435.860501},
	series = {{SIGIR} '03},
	abstract = {Soboroff, Nicholas and Cahan recently proposed a method for evaluating the performance of retrieval systems without relevance judgments. They demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the {TREC} competition. In this work, we propose an explanation for this phenomenon. We devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results. Then, given a collection of retrieval systems and their retrieved results, we use this measure to assess the average similarity of a system to the other systems in the collection. We demonstrate that evaluating retrieval systems according to average similarity yields results quite similar to the methodology proposed by Soboroff et{\textbackslash}textasciitildeal., and we further demonstrate that these two techniques are in fact highly correlated. Thus, the techniques are effectively evaluating and ranking retrieval systems by "popularity" as opposed to "performance.},
	pages = {361--362},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Savell, Robert},
	urldate = {2013-07-11},
	date = {2003},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {retrieval, ranking, systems},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/9HTL7TPU/Aslam and Savell - 2003 - On the effectiveness of evaluating retrieval syste.pdf:application/pdf}
}

@inproceedings{aslam_unified_2003,
	title = {A unified model for metasearch, pooling, and system evaluation},
	isbn = {1-58113-723-0},
	url = {http://doi.acm.org/10.1145/956863.956953},
	doi = {10.1145/956863.956953},
	series = {{CIKM} '03},
	abstract = {We present a unified model which, given the ranked lists of documents returned by multiple retrieval systems in response to a given query, simultaneously solves the problems of (1) fusing the ranked lists of documents in order to obtain a high-quality combined list (metasearch); (2) generating document collections likely to contain large fractions of relevant documents (pooling); and (3) accurately evaluating the underlying retrieval systems with small numbers of relevance judgments (efficient system assessment). Our approach is based on the Hedge algorithm for on-line learning. In effect, our proposed system "learns" which documents are likely to be relevant from a sequence of on-line relevance judgments. In experiments using {TREC} data, our methodology is shown to outperform standard methods for metasearch, pooling, and system evaluation, often remarkably so.},
	pages = {484--491},
	booktitle = {Proceedings of the twelfth international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Pavlu, Virgiliu and Savell, Robert},
	urldate = {2013-07-11},
	date = {2003},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {pooling, evaluation, active learning, metasearch},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/43UZT66A/Aslam et al. - 2003 - A unified model for metasearch, pooling, and syste.pdf:application/pdf}
}

@inproceedings{carterette_evaluation_2008,
	title = {Evaluation over thousands of queries},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390445},
	doi = {10.1145/1390334.1390445},
	series = {{SIGIR} '08},
	abstract = {Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness. There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available. In light of this, it should be possible to evaluate over many more queries without much more total judging effort. The Million Query Track at {TREC} 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries. We present results of the track, along with deeper analysis: investigating tradeoffs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more cost-effective and as reliable as fewer queries with more judgments. Total assessor effort can be reduced by 95\% with no appreciable increase in evaluation errors.},
	pages = {651--658},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A. and Allan, James},
	urldate = {2013-07-11},
	date = {2008},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {test collections, evaluation, information retrieval, million query track},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/MRMN5CDZ/Carterette et al. - 2008 - Evaluation over thousands of queries.pdf:application/pdf}
}

@inproceedings{yilmaz_simple_2008,
	title = {A simple and efficient sampling method for estimating {AP} and {NDCG}},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390437},
	doi = {10.1145/1390334.1390437},
	series = {{SIGIR} '08},
	abstract = {We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., {infAP} [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for {infAP}, (2) we extend {infAP} to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate {nDCG} from incomplete judgments. We validate the proposed methods using {TREC} data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in {TREC} Terabyte track '06.},
	pages = {603--610},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Kanoulas, Evangelos and Aslam, Javed A.},
	urldate = {2013-07-11},
	date = {2008},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {sampling, evaluation, average precision, incomplete judgments, {infAP}, {nDCG}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/KFQBZ7J7/Yilmaz et al. - 2008 - A simple and efficient sampling method for estimat.pdf:application/pdf}
}

@inproceedings{zobel_how_1998,
	title = {How reliable are the results of large-scale information retrieval experiments?},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291014},
	doi = {10.1145/290941.291014},
	series = {{SIGIR} '98},
	abstract = {Two stages in measurement of techniques for information retrieval are gathering of documents for relevance assessment and use of the assessments to numerically evaluate effectiveness. We consider both of these stages in the context of the {TREC} experiments, to determine whether they lead to measurements that are trustworthy and fair. Our detailed empirical investigation of the {TREC} results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found. We propose a new pooling strategy that can significantly in- crease the number of relevant documents found for given effort, without compromising fairness.},
	pages = {307--314},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Zobel, Justin},
	urldate = {2013-07-11},
	date = {1998},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/FCQJAT6W/Zobel - 1998 - How reliable are the results of large-scale inform.pdf:application/pdf}
}

@inproceedings{bernstein_redundant_2005,
	title = {Redundant documents and search effectiveness},
	isbn = {1-59593-140-6},
	url = {http://doi.acm.org/10.1145/1099554.1099733},
	doi = {10.1145/1099554.1099733},
	series = {{CIKM} '05},
	abstract = {The web contains a great many documents that are content-equivalent, that is, informationally redundant with respect to each other. The presence of such mutually redundant documents in search results can degrade the user search experience. Previous attempts to address this issue, most notably the {TREC} novelty track, were characterized by difficulties with accuracy and evaluation. In this paper we explore syntactic techniques — particularly document fingerprinting — for detecting content equivalence. Using these techniques on the {TREC} {GOV}1 and {GOV}2 corpora revealed a high degree of redundancy; a user study confirmed that our metrics were accurately identifying content-equivalence. We show, moreover, that content-equivalent documents have a significant effect on the search experience: we found that 16.6\% of all relevant documents in runs submitted to the {TREC} 2004 terabyte track were redundant.},
	pages = {736--743},
	booktitle = {Proceedings of the 14th {ACM} international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Bernstein, Yaniv and Zobel, Justin},
	urldate = {2013-07-11},
	date = {2005},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {duplicate detection, novelty, search effectiveness},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/IZQ35DHI/Bernstein and Zobel - 2005 - Redundant documents and search effectiveness.pdf:application/pdf}
}

@inproceedings{voorhees_philosophy_2002,
	title = {The Philosophy of Information Retrieval Evaluation},
	isbn = {3-540-44042-9},
	url = {http://dl.acm.org/citation.cfm?id=648264.753539},
	series = {{CLEF} '01},
	pages = {355--370},
	booktitle = {Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems},
	publisher = {Springer-Verlag},
	author = {Voorhees, Ellen M.},
	urldate = {2013-07-11},
	date = {2002},
	note = {event-place: London, {UK}, {UK}}
}

@inproceedings{voorhees_variations_1998,
	title = {Variations in relevance judgments and the measurement of retrieval effectiveness},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291017},
	doi = {10.1145/290941.291017},
	series = {{SIGIR} '98},
	pages = {315--323},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Voorhees, Ellen M.},
	urldate = {2013-07-11},
	date = {1998},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/BUZWRK2C/Voorhees - 1998 - Variations in relevance judgments and the measurem.pdf:application/pdf}
}

@inproceedings{buckley_evaluating_2000,
	title = {Evaluating evaluation measure stability},
	isbn = {1-58113-226-3},
	url = {http://doi.acm.org/10.1145/345508.345543},
	doi = {10.1145/345508.345543},
	series = {{SIGIR} '00},
	abstract = {This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.},
	pages = {33--40},
	booktitle = {Proceedings of the 23rd annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Buckley, Chris and Voorhees, Ellen M.},
	urldate = {2013-07-11},
	date = {2000},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/NPR2GBWI/Buckley and Voorhees - 2000 - Evaluating evaluation measure stability.pdf:application/pdf}
}

@article{buckley_bias_2007,
	title = {Bias and the limits of pooling for large collections},
	volume = {10},
	issn = {1386-4564},
	url = {http://dx.doi.org/10.1007/s10791-007-9032-x},
	doi = {10.1007/s10791-007-9032-x},
	abstract = {Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic. The idea behind pooling is to find enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased. Yet a constant-size pool represents an increasingly small percentage of the document set as document sets grow larger, and at some point the assumption of approximately complete judgments must become invalid. This paper shows that the judgment sets produced by traditional pooling when the pools are too small relative to the total document set size can be biased in that they favor relevant documents that contain topic title words. This phenomenon is wholly dependent on the collection size and does not depend on the number of relevant documents for a given topic. We show that the {AQUAINT} test collection constructed in the recent {TREC} 2005 workshop exhibits this biased relevance set; it is likely that the test collections based on the much larger {GOV}2 document set also exhibit the bias. The paper concludes with suggested modifications to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built.},
	pages = {491--508},
	number = {6},
	journaltitle = {Inf. Retr.},
	author = {Buckley, Chris and Dimmick, Darrin and Soboroff, Ian and Voorhees, Ellen},
	urldate = {2013-07-11},
	date = {2007-12},
	keywords = {pooling, test collections, Sampling bias}
}

@inproceedings{soboroff_ranking_2001,
	title = {Ranking retrieval systems without relevance judgments},
	isbn = {1-58113-331-6},
	url = {http://doi.acm.org/10.1145/383952.383961},
	doi = {10.1145/383952.383961},
	series = {{SIGIR} '01},
	abstract = {The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics. It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text {REtrieval} Conference ({TREC}) has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems. Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to aspseudo-relevance judgments.Rankings of systems with our methodology correlate positively with official {TREC} rankings, although the performance of the top systems is not predicted well. The correlations are stable over a variety of pool depths and sampling techniques. With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.},
	pages = {66--73},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian and Nicholas, Charles and Cahan, Patrick},
	urldate = {2013-07-11},
	date = {2001},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/LNCDL79K/Soboroff et al. - 2001 - Ranking retrieval systems without relevance judgme.pdf:application/pdf}
}

@inproceedings{soboroff_evaluating_2004,
	title = {On evaluating web search with very few relevant documents},
	isbn = {1-58113-881-4},
	url = {http://doi.acm.org/10.1145/1008992.1009105},
	doi = {10.1145/1008992.1009105},
	series = {{SIGIR} '04},
	abstract = {Many common web searches by their nature have a very small number of relevant documents. Homepage and "namedpage" searching are known-item searches where there is only a single relevant document. Topic distillation is a special kind of topical relevance search where the user wishes to find a few key web sites rather than every relevant web page. Because these types of searches are so common, web search evaluations have come to focus on tasks where there are very few relevant documents. Evaluations with few relevant documents pose special challenges for current metrics. In particular, the {TREC} 2003 topic distillation evaluation is unable to distinguish most submitted runs from each other.},
	pages = {530--531},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	urldate = {2013-07-11},
	date = {2004},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {measurement error, web search},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/MFY95YTB/Soboroff - 2004 - On evaluating web search with very few relevant do.pdf:application/pdf}
}

@inproceedings{buttcher_reliable_2007,
	title = {Reliable information retrieval evaluation with incomplete and biased judgements},
	isbn = {978-1-59593-597-7},
	url = {http://doi.acm.org/10.1145/1277741.1277755},
	doi = {10.1145/1277741.1277755},
	series = {{SIGIR} '07},
	abstract = {Information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents. This may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique. We examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgements from the original, biased judgements obtained through pooling. We compare the performance of this method with other approaches to the problem of incomplete judgements, such as bpref, and show that the proposed method leads to higher evaluation accuracy, especially if the set of manual judgements is rich in documents, but highly biased against some systems.},
	pages = {63--70},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Büttcher, Stefan and Clarke, Charles L. A. and Yeung, Peter C. K. and Soboroff, Ian},
	urldate = {2013-07-11},
	date = {2007},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {evaluation, information retrieval, incomplete judgments},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/D5LXELUX/Büttcher et al. - 2007 - Reliable information retrieval evaluation with inc.pdf:application/pdf}
}

@inproceedings{bailey_relevance_2008,
	title = {Relevance assessment: are judges exchangeable and does it matter},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390447},
	doi = {10.1145/1390334.1390447},
	series = {{SIGIR} '08},
	shorttitle = {Relevance assessment},
	abstract = {We investigate to what extent people making relevance judgements for a reusable {IR} test collection are exchangeable. We consider three classes of judge: "gold standard" judges, who are topic originators and are experts in a particular information seeking task; "silver standard" judges, who are task experts but did not create topics; and "bronze standard" judges, who are those who did not define topics and are not experts in the task. Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold standard judges are preferred.},
	pages = {667--674},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Bailey, Peter and Craswell, Nick and Soboroff, Ian and Thomas, Paul and de Vries, Arjen P. and Yilmaz, Emine},
	urldate = {2013-07-11},
	date = {2008},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {inter-rater agreement, test collection relevance judgements},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/24DKGKM9/Bailey et al. - 2008 - Relevance assessment are judges exchangeable and .pdf:application/pdf}
}

@inproceedings{soboroff_building_2003,
	title = {Building a filtering test collection for {TREC} 2002},
	isbn = {1-58113-646-3},
	url = {http://doi.acm.org/10.1145/860435.860481},
	doi = {10.1145/860435.860481},
	series = {{SIGIR} '03},
	abstract = {Test collections for the filtering track in {TREC} have typically used either past sets of relevance judgments, or categorized collections such as Reuters Corpus Volume 1 or {OHSUMED}, because filtering systems need relevance judgments during the experiment for training and adaptation. For {TREC} 2002, we constructed an entirely new set of search topics for the Reuters Corpus for measuring filtering systems. Our method for building the topics involved multiple iterations of feedback from assessors, and fusion of results from multiple search systems using different search algorithms. We also developed a second set of "inexpensive" topics based on categories in the document collection. We found that the initial judgments made for the experiment were sufficient; subsequent pooled judging changed system rankings very little. We also found that systems performed very differently on the category topics than on the assessor-built topics.},
	pages = {243--250},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian and Robertson, Stephen},
	urldate = {2013-07-11},
	date = {2003},
	note = {event-place: New York, {NY}, {USA}},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/UU6WICQL/Soboroff and Robertson - 2003 - Building a filtering test collection for TREC 2002.pdf:application/pdf}
}

@report{gilbert_statistical_1979,
	title = {Statistical Bases of Relevance Assessment for the '{IDEAL}' Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {5481},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Gilbert, H and Sparck Jones, Karen},
	urldate = {2013-07-11},
	date = {1979}
}

@report{sparck_jones_report_1977,
	title = {Report on a Design Study for the '{IDEAL}' Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {5428},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Sparck Jones, Karen and Bates, R. G.},
	urldate = {2013-07-11},
	date = {1977}
}

@report{sparck_jones_report_1975,
	title = {Report on the Need for and Provision for an '{IDEAL}' Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {British Library Report 5266},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Sparck Jones, Karen and van Rijsbergen, C. J.},
	urldate = {2013-07-11},
	date = {1975},
	file = {ACM SIGIR - Museum Contents:/Users/soboroff/Zotero/storage/PAQN5TUX/allcontents.html:text/html}
}

@article{sanderson_test_2010,
	title = {Test Collection Based Evaluation of Information Retrieval Systems},
	volume = {4},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-information-retrieval/INR-009},
	doi = {10.1561/1500000009},
	pages = {247--375},
	number = {4},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	author = {Sanderson, Mark},
	urldate = {2013-07-11},
	date = {2010},
	file = {now publishers – Test Collection Based Evaluation of Information Retrieval Systems:/Users/soboroff/Zotero/storage/NRS9RZWF/INR-009.html:text/html}
}

@collection{voorhees_trec:_2005,
	title = {{TREC}: Experiment and Evaluation in Information Retrieval},
	isbn = {978-0-262-22073-6},
	url = {http://mitpress.mit.edu/books/trec},
	abstract = {The Text {REtrieval} Conference ({TREC}), a yearly workshop hosted by the {US} government's National Institute of Standards and Technology, provides the infrastructure necessary for large-scale evaluation of text retrieval methodologies. With the goal of accelerating research in this area, {TREC} created the first large test collections of full-text documents and standardized retrieval evaluation. The impact has been significant; since {TREC}'s beginning in 1992, retrieval effectiveness has approximately doubled. {TREC} has built a variety of large test collections, including collections for such specialized retrieval tasks as cross-language retrieval and retrieval of speech. Moreover, {TREC} has accelerated the transfer of research ideas into commercial systems, as demonstrated in the number of retrieval techniques developed in {TREC} that are now used in Web search engines. This book provides a comprehensive review of {TREC} research, summarizing the variety of {TREC} results, documenting the best practices in experimental information retrieval, and suggesting areas for further research. The first part of the book describes {TREC}'s history, test collections, and retrieval methodology. Next, the book provides "track" reports—describing the evaluations of specific tasks, including routing and filtering, interactive retrieval, and retrieving noisy text. The final part of the book offers perspectives on {TREC} from such participants as Microsoft Research, University of Massachusetts, Cornell University, University of Waterloo, City University of New York, and {IBM}. The book will be of interest to researchers in information retrieval and related technologies, including natural language processing.},
	publisher = {{MIT} Press},
	editor = {Voorhees, Ellen M. and Harman, Donna K.},
	date = {2005}
}

@online{singhal_introducing_2012,
	title = {Introducing the Knowledge Graph: things, not strings},
	url = {https://www.blog.google/products/search/introducing-knowledge-graph-things-not/},
	shorttitle = {Introducing the Knowledge Graph},
	abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
	author = {Singhal, Amit},
	urldate = {2019-01-03},
	date = {2012-05-16},
	file = {Snapshot:/Users/soboroff/Zotero/storage/AJBCHZDG/introducing-knowledge-graph-things-not.html:text/html}
}

@article{harman_darpa_1992,
	title = {The {DARPA} {TIPSTER} Project},
	volume = {26},
	issn = {0163-5840},
	url = {http://doi.acm.org/10.1145/146565.146567},
	doi = {10.1145/146565.146567},
	abstract = {This note is the first of four papers in this issue describing the ongoing work connected with the {DARPA} {TIPSTER} Project. The note provides an overview of the project, and the next papers by three of the contractors involved in the project provide some details on the systems involved, and some of the initial results.The {TIPSTER} project is sponsored by the Software and Intelligent Systems Technology Office of the Defense Advanced Research Projects Agency ({DARPA}/{SISTO}) in an effort to significantly advance the state of the art in effective document detection (information retrieval) and data extraction from large, real-world data collections. The first two-year phase of the program is concerned with the development of algorithms for document retrieval, document routing, and data extraction that are both domain and language independent. A call for proposals was made in June of 1990, and contracts for the six participating groups were let in the fall of 1991. Three meetings have been held so far, with the first results presented in September of 1992.There are two separate, but connected parts of {TIPSTER}. The first part of the project, document detection, is concerned with retrieving relevant documents" from very large (3 gigabyte) collections of documents, both in a routing environment, and in an adhoc retrieval environment. The routing environment is similar to the document filtering or profile searches currently done in libraries, where a query topic is constant, and the documents are viewed as the incoming stream of publications. The adhoc part of the project is similar to the standard search done against static collections.The second part of the {TIPSTER} project is concerned with data extraction. Here it is assumed that there is a much smaller set of documents, presumed to be mostly relevant to a topic, and the goal is to extract information to fill a database. This database could then be used for many applications, such as question-answering systems, report writing, or data analysis. The data extraction part of {TIPSTER} is being done by groups using natural language understanding techniques, and this part will not be described in this issue.},
	pages = {26--28},
	number = {2},
	journaltitle = {{SIGIR} Forum},
	author = {Harman, Donna},
	urldate = {2018-12-30},
	date = {1992-10},
	file = {ACM Full Text PDF:/Users/soboroff/Zotero/storage/RKMS9ZAG/Harman - 1992 - The DARPA TIPSTER Project.pdf:application/pdf}
}

@inproceedings{turpin_why_2001,
	title = {Why Batch and User Evaluations Do Not Give the Same Results},
	isbn = {978-1-58113-331-8},
	url = {http://doi.acm.org/10.1145/383952.383992},
	doi = {10.1145/383952.383992},
	series = {{SIGIR} '01},
	abstract = {Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the {TREC} Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.},
	pages = {225--231},
	booktitle = {Proceedings of the 24th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Turpin, Andrew H. and Hersh, William},
	urldate = {2018-12-30},
	date = {2001},
	note = {event-place: New York, {NY}, {USA}},
	file = {Submitted Version:/Users/soboroff/Zotero/storage/LZKXKIBN/Turpin and Hersh - 2001 - Why Batch and User Evaluations Do Not Give the Sam.pdf:application/pdf}
}

@inproceedings{voorhees_evaluation_2001,
	location = {New Orleans, Louisiana, United States},
	title = {Evaluation by highly relevant documents},
	isbn = {978-1-58113-331-8},
	url = {http://portal.acm.org/citation.cfm?doid=383952.383963},
	doi = {10.1145/383952.383963},
	abstract = {Given the size of the w eb,the search engine industry has argued that engines should be evaluated by their ability to retrieve highly relevant pages rather than all possible relevant pages. T o explore the role highly relevant documents play in retrieval system evaluation, assessors for the {TREC}-9 w eb track used a three-point relevance scale and also selected best pages for each topic. The relative e ectiv eness of runs evaluated by di erent relevant document sets differed, con rming the hypothesis that di erent retrieval techniques work better for retrieving highly relevant documents. Yet evaluating by highly relevant documents can be unstable since there are relatively few highly relevant documents. {TREC} assessors frequently disagreed in their selection of the best page, and subsequent evaluation by best page across di erent assessors varied widely. The discounted cumulative gain measure introduced by Jarvelin and Kekalainen increases ev aluation stabiliyt by incorporating all relevance judgments while still giving precedence to highly relevant documents.},
	eventtitle = {{SIGIR} 2001},
	pages = {74--82},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM} Press},
	author = {Voorhees, Ellen M.},
	urldate = {2019-08-26},
	date = {2001},
	file = {Voorhees - 2001 - Evaluation by highly relevant documents.pdf:/Users/soboroff/Zotero/storage/IUNQMUJ2/Voorhees - 2001 - Evaluation by highly relevant documents.pdf:application/pdf}
}

@article{hayes_answering_2007,
	title = {Answering the Call for a Standard Reliability Measure for Coding Data},
	volume = {1},
	issn = {1931-2458, 1931-2466},
	url = {http://www.tandfonline.com/doi/abs/10.1080/19312450709336664},
	doi = {10.1080/19312450709336664},
	pages = {77--89},
	number = {1},
	journaltitle = {Communication Methods and Measures},
	shortjournal = {Communication Methods and Measures},
	author = {Hayes, Andrew F. and Krippendorff, Klaus},
	urldate = {2019-12-19},
	date = {2007-04},
	langid = {english},
	file = {Hayes and Krippendorff - 2007 - Answering the Call for a Standard Reliability Meas.pdf:/Users/soboroff/Zotero/storage/RYJ5FTSH/Hayes and Krippendorff - 2007 - Answering the Call for a Standard Reliability Meas.pdf:application/pdf}
}

@article{kelly_methods_2009,
	title = {Methods for Evaluating Interactive Information Retrieval Systems with Users},
	volume = {3},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/article/Details/INR-012},
	doi = {10.1561/1500000012},
	abstract = {This paper provides overview and instruction regarding the evaluation of interactive information retrieval systems with users. The primary goal of this article is to catalog and compile material related to this topic into a single source. This article (1) provides historical background on the development of user-centered approaches to the evaluation of interactive information retrieval systems; (2) describes the major components of interactive information retrieval system evaluation; (3) describes diﬀerent experimental designs and sampling strategies; (4) presents core instruments and data collection techniques and measures; (5) explains basic data analysis techniques; and (4) reviews and discusses previous studies. This article also discusses validity and reliability issues with respect to both measures and methods, presents background information on research ethics and discusses some ethical issues which are speciﬁc to studies of interactive information retrieval ({IIR}). Finally, this article concludes with a discussion of outstanding challenges and future research directions.},
	pages = {1--224},
	number = {1},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	shortjournal = {{FNT} in Information Retrieval},
	author = {Kelly, Diane},
	urldate = {2020-12-01},
	date = {2009},
	langid = {english},
	file = {Kelly - 2007 - Methods for Evaluating Interactive Information Ret.pdf:/Users/soboroff/Zotero/storage/5HXHII2V/Kelly - 2007 - Methods for Evaluating Interactive Information Ret.pdf:application/pdf}
}

@incollection{cleverdon_chapter_1966,
	location = {College of Aeronautics, Cranfield, England},
	title = {Chapter 3, Documents and questions},
	volume = {1 (Design)},
	url = {https://sigir.org/resources/museum/},
	pages = {19--39},
	booktitle = {Factors determining the performance of indexing systems},
	author = {Cleverdon, Cyril W. and Mills, Jack and Keen, E. Michael},
	date = {1966}
}

@collection{sakai_evaluating_2020,
	title = {Evaluating Information Retrieval and Access Tasks: {NTCIR}’s Legacy of Research Impact},
	series = {Information Retrieval Series},
	number = {43},
	publisher = {Springer},
	editor = {Sakai, Tetsuya and Oard, Douglas and Kando, Noriko},
	date = {2020},
	file = {Sakai et al. - 2020 - Evaluating Information Retrieval and Access Tasks.pdf:/Users/soboroff/Zotero/storage/NYF2NAUG/Sakai et al. - 2020 - Evaluating Information Retrieval and Access Tasks.pdf:application/pdf}
}

@article{jarvelin_cumulated_2002,
	title = {Cumulated gain-based evaluation of {IR} techniques},
	volume = {20},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/582415.582418},
	doi = {10.1145/582415.582418},
	abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop {IR} techniques in this direction, it is necessary to develop evaluation approaches and methods that credit {IR} methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of {IR} techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using {TREC} data: sample system run results for 20 queries in {TREC}-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit {IR} methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance {IR} techniques and allow interpretation, for example, from the user point of view.},
	pages = {422--446},
	number = {4},
	journaltitle = {{ACM} Transactions on Information Systems},
	shortjournal = {{ACM} Trans. Inf. Syst.},
	author = {Järvelin, Kalervo and Kekäläinen, Jaana},
	urldate = {2020-12-30},
	date = {2002-10-01},
	keywords = {cumulated gain, Graded relevance judgments},
	file = {Full Text PDF:/Users/soboroff/Zotero/storage/TM3EDMVC/Järvelin and Kekäläinen - 2002 - Cumulated gain-based evaluation of IR techniques.pdf:application/pdf}
}